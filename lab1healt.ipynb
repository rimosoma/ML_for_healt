{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOujrFxfPc7EWgoER/1zqRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rimosoma/ML_for_healt/blob/main/lab1healt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knT_UBJ39Ttf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SolveMinProbl:\n",
        "    \"\"\"\n",
        "        Vettore dei Pesi Ottimali (w): Il risultato del problema di minimizzazione\n",
        "        Ã¨ il vettore w, di dimensione NfÃ—1. Ogni componente\n",
        "        di questo vettore, wi, rappresenta il peso ottimale assegnato alla i-esima\n",
        "        feature per prevedere il valore in y. Asse Y (w):\n",
        "        Vogliamo visualizzare questi pesi ottimali. Quindi, l'asse Y deve rappresentare\n",
        "        i valori di w0,w1,w2,â€¦.\n",
        "        Asse X (Nf): Per identificare a quale feature appartiene ciascun peso,\n",
        "        l'asse X deve fornire l'indice della feature.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self ,y=np.ones((3,)) ,A=np.eye(3)):   #initialization with parameters useful if the user doesn't put them\n",
        "        self.matr =A                                    # matrix A (known: patients and features)\n",
        "        self.y = y                                      # column vector y (known: observation)\n",
        "        self.Np = y.shape[0]                            # number of rows\n",
        "        self.Nf =A.shape[1]                             # number of columns\n",
        "        self.what = np.zeros((self .Nf,1) ,dtype=float ) # column vector what to be found\n",
        "        self.min = 0                                    # square norm of the error (initially set to zero)\n",
        "        self.steps = 0\n",
        "        self.err = None\n",
        "        return\n",
        "\n",
        "    def plot_what(self , title=\"Solution\"):             #method to plot what over Nf\n",
        "        what=self.what                                  # retrieve what\n",
        "        n=np.arange(self.Nf)                            # instantiate the xâˆ’axis\n",
        "        plt.figure()                                    # instantiate the figure\n",
        "        plt.plot(n,what)                                # generate the plot what versus n\n",
        "        plt.xlabel(\"n\")                                 # set the xâˆ’axis label\n",
        "        plt.ylabel(\"w_hat(n)\")                          # set the yâˆ’axis label\n",
        "        plt.title(title)                                # set the title of the figure\n",
        "        plt.grid()                                      # include grids\n",
        "        plt.show()                                      # show the picture\n",
        "        return\n",
        "\n",
        "    def print_result(self , title ):                    # method to print the result\n",
        "        print( )\n",
        "        print ( title , \" : \")\n",
        "        print (f\"    optimum weight vector: {self.what.T}\")\n",
        "        print (f\"    total steps : {self.steps}\")\n",
        "        print (f\"    final cost : {self.min}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def plot_err(self, title='Square error', logy=0, logx=0):\n",
        "        \"\"\"\n",
        "            Method to plot the squared error versus the iteration index.\n",
        "            Parameters:\n",
        "            title (str): The title of the plot.\n",
        "            logy (int): If 1, the y-axis (error) uses a logarithmic scale.\n",
        "            logx (int): If 1, the x-axis (iterations) uses a logarithmic scale.\n",
        "        \"\"\"\n",
        "        if self.err is None or len(self.err) == 0:\n",
        "            print(\"Errore: self.err non Ã¨ stato calcolato. Niente da plottare.\")\n",
        "            return\n",
        "\n",
        "        err = self.err                                  # Retrieve the matrix with the error values: [[n0, err0], [n1, err1], ...]\n",
        "        plt.figure()                                    # Instantiate the figure\n",
        "        x_data = err[:, 0]                              # Column 0: Iteration index (n) -> X-axis\n",
        "        y_data = err[:, 1]                              # Column 1: Squared error (e(n)) -> Y-axis\n",
        "\n",
        "        # Check scale flags and select the appropriate plotting function\n",
        "        if (logy == 0) and (logx == 0):                 # Natural scales (Linear-Linear)\n",
        "            plt.plot(x_data, y_data)\n",
        "        elif (logy == 1) and (logx == 0):               # Semilogarithmic Y scale (Linear-Log)\n",
        "            plt.semilogy(x_data, y_data)\n",
        "        elif (logy == 0) and (logx == 1):               # Semilogarithmic X scale (Log-Linear)\n",
        "            plt.semilogx(x_data, y_data)\n",
        "        elif (logy == 1) and (logx == 1):               # Log-Log scales\n",
        "            plt.loglog(x_data, y_data)\n",
        "\n",
        "        plt.xlabel('n (Iteration Index)')               # Set the x-axis label\n",
        "        plt.ylabel('e(n) (Squared Error)')              # Set the y-axis label\n",
        "        plt.title(title)                                # Set the title of the figure\n",
        "        plt.margins(0.01, 0.1)                          # Leave some space (margins) around the plot area\n",
        "        plt.grid()                                      # Include grids\n",
        "        plt.show()\n",
        "        return"
      ],
      "metadata": {
        "id": "eUDJ-pSQ978e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SolveSteepestDecent(SolveMinProbl):\n",
        "    \"\"\"\n",
        "    Steepest Descent with Exact Line Search (Optimal Step Size).\n",
        "    Calculates the optimal learning rate (gamma) analytically in every iteration\n",
        "    using the Hessian (H = 2*A^T*A) and the gradient (g).\n",
        "    Formula: gamma = (g^T * g) / (g^T * H * g).\n",
        "    Converges faster than BGD with a fixed gamma, but requires pre-calculating and using the Hessian.\n",
        "    y must be a column vector. If it's not, before call the function, perform y.reshape(-1,1)'\n",
        "    \"\"\"\n",
        "    # EDIT from original for lab1: Added 'tol' (tolerance) and increased default Nit (from 100 to 1000)\n",
        "    def run(self, Nit=1000, tol=10e-10):\n",
        "        self.err = np.empty((0,2) , dtype=float )        # Initialize array to store [iteration_count, total_squared_error]\n",
        "\n",
        "        A = self.matr                                    # Retrieve the data matrix X\n",
        "        y = self.y                                       # Retrieve the target vector y\n",
        "\n",
        "        # EDIT from original for lab1: Changed initialization from random to zero for reproducible results\n",
        "        # w = np.random.rand(self.Nf, 1)                 # Vecchia inizializzazione (casuale)\n",
        "        w = np.zeros((self.Nf, 1))                       # Nuova inizializzazione (deterministica)\n",
        "\n",
        "\n",
        "        # Pre-calculate the Hessian matrix (H)\n",
        "        hessian = 2 * A.T @ A\n",
        "\n",
        "        # Since the cost function is quadratic (least squares), the Hessian is constant\n",
        "        # (it does not depend on w) and can be calculated only once outside the main loop.\n",
        "\n",
        "        sqerr = np.inf # Inizializza l'errore per il caso in cui il loop non parta\n",
        "\n",
        "        for i in range(Nit):                             # Loop for a fixed number of total iterations\n",
        "            # Calculate the FULL Batch Gradient: = 2 * A^T * (A * w - y)\n",
        "            grad = 2 * A.T @ (A @ w - y)\n",
        "\n",
        "            # Calculate the optimal step size (gamma) using the exact line search formula for quadratic functions:\n",
        "            # gamma = (g^T * g) / (g^T * H * g)\n",
        "            numerator = grad.T @ grad\n",
        "            denominator = grad.T @ hessian @ grad\n",
        "\n",
        "            self.steps += 1                              # Increment the step counter\n",
        "\n",
        "            # Stopping condition 1: Check if the denominator is zero (to prevent division error)\n",
        "            if denominator < 1e-10:\n",
        "                break\n",
        "\n",
        "            # Calculate the optimal step size (gamma)\n",
        "            gamma = float(numerator / denominator)\n",
        "\n",
        "            w_new = w - gamma * grad\n",
        "\n",
        "            # EDIT from original for lab1: Added stop condition based on 'tol' tolerance\n",
        "            # Si ferma se la variazione dei pesi (norma L2) Ã¨ trascurabile\n",
        "            if np.linalg.norm(w_new - w) < tol:\n",
        "                w = w_new # Assicura che l'ultimo w calcolato sia salvato\n",
        "                break\n",
        "\n",
        "            w = w_new\n",
        "\n",
        "            # Calculate and store the TOTAL squared error (||Aw - y||^2) on the full dataset\n",
        "            sqerr = np.linalg.norm(A @ w - y)**2\n",
        "            self.err = np.append(self.err, np.array([[i, sqerr]]), axis=0)\n",
        "\n",
        "        self.what = w                                    # Store the final optimized weights vector\n",
        "        self.min = sqerr                                 # Store the final minimum total squared error"
      ],
      "metadata": {
        "id": "XONXJv429cSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "pd.set_option('display.precision', 3)\n",
        "\n",
        "ALL_INFO_FLAG = True\n",
        "\n",
        "# flag to optionally drop the motor_UPDRS regressor (keeps original behaviour)\n",
        "MOTOR_DROP_FLAG = False\n",
        "\n",
        "# flag to enable/disable data shuffling before train/test split\n",
        "SHUFFLE_DATA = True\n",
        "\n",
        "# flag to enable/disable feature normalization\n",
        "NORMALIZE_FEATURES = True"
      ],
      "metadata": {
        "id": "P58XE1Cw9cl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Read the input csv file\n",
        "plt.close('all')                                                    # close all the figures that might still be open from previous runs\n",
        "X=pd.read_csv(\"parkinsons_updrs_av.csv\")                            # read the dataset; x is a Pandas dataframe\n",
        "features=list(X.columns)                                            #  list of features in the dataset\n",
        "subj=pd.unique(X['subject#'])                                       # get unique patient IDs (useful to know how many subjects)\n",
        "# print basic dataset info\n",
        "#STARTING ANALYSIS\n",
        "\n",
        "# Np = number of rows/patients\n",
        "# Nc=number Nf of regressors + 1 (regressand total UPDRS is included)(columns/features)\n",
        "Np,Nc=X.shape\n",
        "\n",
        "if ALL_INFO_FLAG is True:\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print('âœ…ALL_INFO_FLAG: ON; â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸â¬‡ï¸')\n",
        "    print(\" \")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"-------------------ðŸ§ â€‹ BASIC DATABASE INFO ðŸ§ â€‹-----------------------------\")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "    print(\"-------------------------------------------------------------------------\")\n",
        "\n",
        "    print(\"The original dataset shape  is \",X.shape)\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"The number of distinct patients in the dataset is \",len(subj))\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"the original dataset features are \",len(features))\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"HERE THE ORIGINAL FEATURES:â¬‡ï¸â€‹\")\n",
        "\n",
        "    print(features)\n",
        "\n",
        "    #%% Have a look at the dataset\n",
        "    # show summary statistics for each column (count, mean, std, min, max, percentiles)\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"HERE THE ORIGINAL FEATURES WITH THEIR DESCRIPTIONS:â¬‡ï¸â€‹\")\n",
        "    print(X.describe().T)\n",
        "    # show dtypes and non-null counts (helps detect missing data)\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"HERE DEFAULT INFORMATION ABOUT THE ORIGINAL X:â¬‡ï¸â€‹\")\n",
        "    print(X.info())"
      ],
      "metadata": {
        "id": "E5xtL5JJ9cyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%% Measure and show the covariance matrix\n",
        "Xnorm=(X-X.mean())/X.std()                                          # standardize/normalize all columns (zero mean, unit std) so correlations are comparable\n",
        "c=Xnorm.cov()                                                       # covariance matrix of normalized data\n",
        "\n",
        "# show absolute covariance (correlation-like) as an image to spot strong relations\n",
        "plt.figure()\n",
        "plt.matshow(np.abs(c.values),fignum=0)                              # absolute value of corr.coeffs\n",
        "plt.xticks(np.arange(len(features)), features, rotation=90)\n",
        "plt.yticks(np.arange(len(features)), features, rotation=0)\n",
        "plt.colorbar()\n",
        "plt.title('Correlation coefficients of the features')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./corr_coeff.png')                                     # save the figure\n",
        "plt.draw()\n",
        "plt.figure()\n",
        "\n",
        "# plot covariance between each feature and total_UPDRS (single column)\n",
        "c.total_UPDRS.plot()\n",
        "plt.grid()\n",
        "plt.xticks(np.arange(len(features)),features,rotation=90)\n",
        "plt.title('Corr. coeff. between total_UPDRS and the other features')\n",
        "plt.tight_layout()\n",
        "plt.draw()\n",
        "#plt.savefig('./UPDRS_corr_coeff.png')                              # save the figure\n",
        "\n"
      ],
      "metadata": {
        "id": "YavmsAMj9dCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#START PREPARING DATASET\n",
        "\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"-------------------ðŸ§ â€‹ STARTING REGRESSION ðŸ§ â€‹-----------------------------\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "print(\"-------------------------------------------------------------------------\")\n",
        "\n",
        "#%% Shuffle the data, method 2\n",
        "if SHUFFLE_DATA:\n",
        "    Xsh = X.sample(frac=1, replace=False, random_state=310866, axis=0, ignore_index=True)\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print('âœ…â€‹SHUFFLING FLAG: ON (random_state=310866)')\n",
        "else:\n",
        "    Xsh = X.copy()\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print('â—SHUFFLING FLAG: OFF (data kept in original order)')\n",
        "    # Note: dataset may be ordered by subject/time; without shuffling, train/test split may be biased.\n",
        "\n",
        "if ALL_INFO_FLAG is False:\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print('â—ALL_INFO_FLAG: OFF')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%% Start working on training and test subsets\n",
        "\n",
        "# use half the dataset for training and half for testing (simple split for the lab)\n",
        "Ntr=int(Np*0.5)                                                     # number of training points\n",
        "Nte=Np-Ntr                                                          # number of test points\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%% evaluate mean and st.dev. for the training data only\n",
        "\n",
        "#is convenient to work with features with zero-mean and variance one, which typically\n",
        "#reduces numerical problems and speeds up algorithm convergence (once you remove the mean\n",
        "#from all the random variables, including the regressand, the model has obviously intercept\n",
        "#equal to zero). Therefore we first perform normalization (or standardization or scaling) of\n",
        "#the data, by removing the mean and divide by the standard deviation each random variable:\n",
        "\n",
        "# compute training mean and std -> these will be used to normalize both train and test\n",
        "# mm and ss are Series containing one value for each column (e.g., mean of age, mean of sex, etc.).\n",
        "X_tr=Xsh[0:Ntr]                                                     # dataframe that contains only the training data\n",
        "mm=X_tr.mean()                                                      # mean (series) of each feature\n",
        "ss=X_tr.std()                                                       # standard deviation (series) of each feature\n",
        "# store mean and std for the regressand (total_UPDRS) to denormalize predictions later\n",
        "my=mm['total_UPDRS']                                                # mean of regressand/total UPDRS (for later use)\n",
        "sy=ss['total_UPDRS']                                                # st.dev of regressand/total UPDRS (for later use)\n",
        "\n",
        "\n",
        "#%% Generate the normalized/scaled training and test datasets, remove unwanted regressors\n",
        "# Pandas performs \"broadcasting\": it automatically matches each column in Xsh\n",
        "# with the corresponding mean in 'mm' and std in 'ss'.\n",
        "# Effectively: (each_sample_in_Column_i - Mean_i) / Std_i for every column at once.\n",
        "\n",
        "if NORMALIZE_FEATURES is True:\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"âœ…â€‹NORMALIZED FLAG: ON\")\n",
        "    # normalize full shuffled dataset using training statistics (avoid data leakage)\n",
        "    Xsh_norm=(Xsh-mm)/ss                 # normalized data\n",
        "else:\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"â—â€‹NORMALIZED FLAG: OFF\")\n",
        "    Xsh_norm=Xsh\n",
        "# keep the normalized target separately\n",
        "ysh_norm=Xsh_norm['total_UPDRS']\n",
        "                         # regressand only\n",
        "# drop the target and non-regressor columns before fitting\n",
        "Xsh_norm=Xsh_norm.drop(['total_UPDRS','subject#', 'Shimmer:DDA', 'Jitter:DDP'],axis=1)  # regressors only\n",
        "\n",
        "\n",
        "# optionally drop motor_UPDRS regressor if flag is set (keeps original code behaviour)\n",
        "if MOTOR_DROP_FLAG:\n",
        "    Xsh_norm=Xsh_norm.drop(['motor_UPDRS'],axis=1)                  # regressors only\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"â—â€‹â€‹MOTOR DROP FLAG: ON\")\n",
        "else:\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"---------------------------------------\")\n",
        "    print(\"âœ…MOTOR DROP FLAG: OFF\")\n",
        "\n",
        "#%% Define the list of regressors\n",
        "regressors=list(Xsh_norm.columns)\n",
        "Nf = len(regressors)                                                # number of regressors\n",
        "print(\"---------------------------------------\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"The new regressors are: \",len(regressors))\n",
        "#print(regressors)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert dataframes/series to numpy arrays for linear algebra operations\n",
        "Xsh_norm=Xsh_norm.values                                            # from dataframe to Ndarray\n",
        "ysh_norm=ysh_norm.values                                            # from dataframe to Ndarray\n",
        "# split normalized arrays into training and test parts using earlier indices\n",
        "X_tr_norm=Xsh_norm[0:Ntr]                                           # regressors for training phase\n",
        "X_te_norm=Xsh_norm[Ntr:]                                            # regressors for test phase\n",
        "y_tr_norm=ysh_norm[0:Ntr]                                           # regressand for training phase\n",
        "y_te_norm=ysh_norm[Ntr:]                                            # regressand for test phase\n",
        "print(\"---------------------------------------\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"The dimension of the training normalized subset are:\",X_tr_norm.shape)\n",
        "print(\"The dimension of the test normalized subset are:\",X_te_norm.shape)\n",
        "\n",
        "\n",
        "#FINISH PREPARATIONS"
      ],
      "metadata": {
        "id": "og_WjQGh9dOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#START REGRESSION\n",
        "\n",
        "\n",
        "#FIRST METHOD\n",
        "#%% LLS regression\n",
        "# closed-form linear least squares: w = (X^T X)^{-1} X^T y\n",
        "w_hat_LLS=np.linalg.inv(X_tr_norm.T@X_tr_norm)@(X_tr_norm.T@y_tr_norm)\n",
        "y_hat_tr_norm_LLS=X_tr_norm@w_hat_LLS\n",
        "y_hat_te_norm_LLS=X_te_norm@w_hat_LLS\n",
        "\n",
        "\n",
        "\n",
        "#SECOND METHOD\n",
        "#%% STEEPEST regression\n",
        "# use the provided SolveSteepestDecent class -> iterative gradient-based method\n",
        "s = SolveSteepestDecent(y_tr_norm.reshape(-1,1),X_tr_norm)\n",
        "s.run()\n",
        "print(\"---------------------------------------\")\n",
        "print(\"---------------------------------------\")\n",
        "s.print_result (\"STEEPEST_DECENT\")\n",
        "s.plot_err(\"ERR_STEEP_DESC\")\n",
        "w_hat_steep = s.what.copy()\n",
        "y_hat_tr_norm_steep=X_tr_norm@w_hat_steep\n",
        "y_hat_te_norm_steep=X_te_norm@w_hat_steep\n",
        "\n",
        "\n",
        "#Note that y_te_norm and y_hat_te_norm_ are normalized, and therefore the value of MSE (mean\n",
        "#Square Error) does not say much to a medical doctor (what is the unit of measurement of the\n",
        "#normalized mean square error?), and it is necessary to first de-normalizeË†\n",
        "#y to get a meaningful mean square error:\n",
        "\n",
        "#%% de-normalize data realted to total-UPDRS (the regressand)\n",
        "# bring predictions and true labels back to original scale: y = y_norm * sy + my\n",
        "y_tr_DENORM=y_tr_norm*sy+my\n",
        "y_te_DENORM=y_te_norm*sy+my\n",
        "\n",
        "y_hat_tr_LLS_DENORM=y_hat_tr_norm_LLS*sy+my\n",
        "y_hat_te_LLS_DENORM=y_hat_te_norm_LLS*sy+my\n",
        "\n",
        "y_hat_tr_steep_DENORM=y_hat_tr_norm_steep*sy+my\n",
        "y_hat_te_steep_DENORM=y_hat_te_norm_steep*sy+my\n",
        "\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"---------VETTORE DEI PESI w^-----------\")\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "print(\"Pesi LLS: \", w_hat_LLS)\n",
        "print(\"---------------------------------------\")\n",
        "print(\"Pesi Steepest Descent: \", w_hat_steep.T)\n",
        "print(\"---------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "#FINISH REGRESSION"
      ],
      "metadata": {
        "id": "5ihgjT0g_vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#START PLOTTING LLS\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%% plot the optimum weight vector for LLS\n",
        "# visualize learned weights to see which regressors are important\n",
        "nn=np.arange(Nf)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(nn,w_hat_LLS,'-o')\n",
        "ticks=nn\n",
        "plt.xticks(ticks, regressors, rotation=90)\n",
        "plt.ylabel(r'$\\^w(n)$')\n",
        "plt.title('LLS-Optimized weights')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./LLS-what.png')\n",
        "plt.draw()\n",
        "\n",
        "#%% plot the error histograms for LLS\n",
        "# compute residuals and show distribution for train vs test\n",
        "E_tr_LLS=(y_tr_DENORM-y_hat_tr_LLS_DENORM)# errors on the training dataset\n",
        "E_te_LLS=(y_te_DENORM-y_hat_te_LLS_DENORM)# errors on the test dataset\n",
        "M=np.max([np.max(E_tr_LLS),np.max(E_te_LLS)])\n",
        "m=np.min([np.min(E_tr_LLS),np.min(E_te_LLS)])\n",
        "common_bins=np.arange(m,M,(M-m)/50)\n",
        "e=[E_tr_LLS,E_te_LLS]\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(e,bins=common_bins,density=True, histtype='bar',label=['training','test'])\n",
        "plt.xlabel(r'$e=y-\\^y$')\n",
        "plt.ylabel(r'$P(e$ in bin$)$')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('LLS-Error histograms using all the training dataset')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./LLS-hist.png')\n",
        "plt.draw()\n",
        "\n",
        "\n",
        "#Clearly, the error does not have a Gaussian pdf, it looks like a mixture of two Gaussian pdfs,\n",
        "#and there is not much difference in the training and test subsets (which means that there is no\n",
        "#overfitting)\n",
        "\n",
        "#%% plot the regression lines for  LLS\n",
        "# scatter predicted vs true on test set; ideal line is y_hat=y\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(y_te_DENORM,y_hat_te_LLS_DENORM,'.',label='all')\n",
        "plt.legend()\n",
        "v=plt.axis()\n",
        "plt.plot([v[0],v[1]],[v[0],v[1]],'r',linewidth=2)\n",
        "plt.xlabel(r'$y$')\n",
        "plt.axis('square')\n",
        "plt.ylabel(r'$\\^y$')\n",
        "plt.grid()\n",
        "plt.title('LLS-test')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./LLS-yhat_vs_y.png')\n",
        "plt.draw()\n",
        "\n",
        "#The estimated valuesË†\n",
        "#yare close to the true values yapart from some cases in which yis very\n",
        "#large andË†\n",
        "#ytakes smaller values with an error around 7-8 UPDRS points\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#%% statistics of the errors of LLS\n",
        "# compute many common metrics: min, max, mean, std, MSE, R^2, correlation\n",
        "# on the training dataset:\n",
        "E_tr_max_LLS=E_tr_LLS.max()# max error\n",
        "E_tr_min_LLS=E_tr_LLS.min()# min error\n",
        "E_tr_mu_LLS=E_tr_LLS.mean()# mean error\n",
        "E_tr_sig_LLS=E_tr_LLS.std()# standard deviation of the error\n",
        "E_tr_MSE_LLS=np.mean(E_tr_LLS**2)# mean square error\n",
        "R2_tr_LLS=1-E_tr_MSE_LLS/(np.var(y_tr_DENORM))# coefficient of determination\n",
        "# correlation coefficient:\n",
        "c_tr_LLS=np.mean((y_tr_DENORM-y_tr_DENORM.mean())*(y_hat_tr_LLS_DENORM - y_hat_tr_LLS_DENORM.mean()))/(y_tr_DENORM.std()*y_hat_tr_LLS_DENORM.std())\n",
        "# on the test dataset\n",
        "E_te_max_LLS=E_te_LLS.max()# max error\n",
        "E_te_min_LLS=E_te_LLS.min()# min error\n",
        "E_te_mu_LLS=E_te_LLS.mean()# mean error\n",
        "E_te_sig_LLS=E_te_LLS.std()# standard deviation of the error\n",
        "E_te_MSE_LLS=np.mean(E_te_LLS**2)# mean square error\n",
        "R2_te_LLS=1-E_te_MSE_LLS/(np.var(y_te_DENORM))# coefficient of determination\n",
        "# correlation coefficient:\n",
        "c_te_LLS=np.mean((y_te_DENORM-y_te_DENORM.mean())*(y_hat_te_LLS_DENORM - y_hat_te_LLS_DENORM.mean()))/(y_te_DENORM.std()*y_hat_te_LLS_DENORM.std())\n",
        "cols=['min','max','mean','std','MSE','R^2','corr_coeff']\n",
        "rows=['Training','test']\n",
        "p=np.array([\n",
        "    [E_tr_min_LLS,E_tr_max_LLS,E_tr_mu_LLS,E_tr_sig_LLS,E_tr_MSE_LLS,R2_tr_LLS,c_tr_LLS],\n",
        "    [E_te_min_LLS,E_te_max_LLS,E_te_mu_LLS,E_te_sig_LLS,E_te_MSE_LLS,R2_te_LLS,c_te_LLS],\n",
        "            ])\n",
        "\n",
        "results=pd.DataFrame(p,columns=cols,index=rows)\n",
        "print(results)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#FINISH PLOTTING LLS"
      ],
      "metadata": {
        "id": "AbzDNLMU-359"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#START PLOTTING STEEPEST\n",
        "\n",
        "#%% plot the optimum weight vector for Steepest (I don't use the method of lab0)\n",
        "nn=np.arange(Nf)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(nn,w_hat_steep,'-o')\n",
        "ticks=nn\n",
        "plt.xticks(ticks, regressors, rotation=90)\n",
        "plt.ylabel(r'$\\^w(n)$')\n",
        "plt.title('Steepest Descent weights')\n",
        "plt.grid()\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./STEEPDESC-what.png')\n",
        "plt.draw()\n",
        "\n",
        "#%% plot the error histograms for STEEPEST\n",
        "E_tr_steep =(y_tr_DENORM-y_hat_tr_steep_DENORM.ravel())# errors on the training dataset ravel perche senno era diff tra array e vettore\n",
        "E_te_steep =(y_te_DENORM-y_hat_te_steep_DENORM.ravel())# errors on the test dataset\n",
        "\n",
        "M=np.max([np.max(E_tr_steep),np.max(E_te_steep)])\n",
        "m=np.min([np.min(E_tr_steep),np.min(E_te_steep)])\n",
        "common_bins=np.arange(m,M,(M-m)/50)\n",
        "e = [E_tr_steep, E_te_steep]\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(e,bins=common_bins,density=True, histtype='bar',label=['training','test'])\n",
        "plt.xlabel(r'$e=y-\\^y$')\n",
        "plt.ylabel(r'$P(e$ in bin$)$')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.title('STEEPEST-Error histograms using all the training dataset')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./STEEPEST-hist.png')\n",
        "plt.draw()\n",
        "\n",
        "\n",
        "print(np.allclose(E_tr_LLS, E_tr_steep, atol=1e-6))\n",
        "\n",
        "#%% plot the regression lines for  STEEPEST\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.plot(y_te_DENORM,y_hat_te_steep_DENORM,'.',label='all')\n",
        "plt.legend()\n",
        "v=plt.axis()\n",
        "plt.plot([v[0],v[1]],[v[0],v[1]],'r',linewidth=2)\n",
        "plt.xlabel(r'$y$')\n",
        "plt.axis('square')\n",
        "plt.ylabel(r'$\\^y$')\n",
        "plt.grid()\n",
        "plt.title('STEEP-test')\n",
        "plt.tight_layout()\n",
        "#plt.savefig('./STEEP-yhat_vs_y.png')\n",
        "plt.draw()\n",
        "\n",
        "#%% statistics of the errors of STEEP\n",
        "# compute the same set of metrics for steepest-descent results\n",
        "# on the training dataset:\n",
        "E_tr_max=E_tr_steep.max()# max error\n",
        "E_tr_min=E_tr_steep.min()# min error\n",
        "E_tr_mu=E_tr_steep.mean()# mean error\n",
        "E_tr_sig=E_tr_steep.std()# standard deviation of the error\n",
        "E_tr_MSE=np.mean(E_tr_steep**2)# mean square error\n",
        "R2_tr=1-E_tr_MSE/(np.var(y_tr_DENORM))# coefficient of determination\n",
        "# correlation coefficient:\n",
        "c_tr=np.mean((y_tr_DENORM-y_tr_DENORM.mean())*(y_hat_tr_steep_DENORM - y_hat_tr_steep_DENORM.mean()))/(y_tr_DENORM.std()*y_hat_tr_steep_DENORM.std())\n",
        "# on the test dataset\n",
        "E_te_max=E_te_steep.max()# max error\n",
        "E_te_min=E_te_steep.min()# min error\n",
        "E_te_mu=E_te_steep.mean()# mean error\n",
        "E_te_sig=E_te_steep.std()# standard deviation of the error\n",
        "E_te_MSE=np.mean(E_te_steep**2)# mean square error\n",
        "R2_te=1-E_te_MSE/(np.var(y_te_DENORM))# coefficient of determination\n",
        "# correlation coefficient:\n",
        "c_te=np.mean((y_te_DENORM-y_te_DENORM.mean())*(y_hat_te_steep_DENORM - y_hat_te_steep_DENORM.mean()))/(y_te_DENORM.std()*y_hat_te_steep_DENORM.std())\n",
        "cols=['min','max','mean','std','MSE','R^2','corr_coeff']\n",
        "rows=['Training','test']\n",
        "p=np.array([\n",
        "    [E_tr_min,E_tr_max,E_tr_mu,E_tr_sig,E_tr_MSE,R2_tr,c_tr],\n",
        "    [E_te_min,E_te_max,E_te_mu,E_te_sig,E_te_MSE,R2_te,c_te],\n",
        "            ])\n",
        "\n",
        "results=pd.DataFrame(p,columns=cols,index=rows)\n",
        "print(results)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4wCVEDV1_DNY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}